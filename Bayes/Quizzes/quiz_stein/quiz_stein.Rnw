\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage{graphicx} % Required to insert images
\usepackage{hyperref}
\usepackage{amsmath} %for binomial pdf
\usepackage{parskip} % so that there's space bw paragraphs
\usepackage{float}
\usepackage{amsfonts}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in 

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{STAT 532: Bayes} % Top left header
\chead{Quiz Stein} % Top center header
\rhead{Andrea Mack} % Top right header
\lfoot{10/27/2016} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs
\setlength\parskip{0.5cm}
\restylefloat{table}

%----------------------------------------------------------------------------------------
%	DOCUMENT STRUCTURE COMMANDS
%	Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1}{}\nobreak
}


%----------------------------------------------------------------------------------------%

\begin{document}

<<setup, include = FALSE>>=
require(xtable)
require(ggplot2)
require(dplyr) #between()
require(truncnorm) #work with truncated norm
require(LearnBayes)
opts_chunk$set(echo = FALSE, warning = FALSE)
@
  
\begin{enumerate}
\item%1
\begin{enumerate}
\item%1a

<<prob1a,include = FALSE>>=
  
x <- read.csv("SteinData.csv", head = T)
head(x)

phat.x <- mean(x$avg45)
sigma2.x <- phat.x*(1-phat.x)/45
c.x <- 1-((dim(x)[1] - 3)*sigma2.x/sum((x$avg45 - phat.x)^2))

z.x <- phat.x + c.x*(x$avg45 - phat.x)


mse.z <- sum((z.x - x$avgSeason)^2)
mse.x <- sum((phat.x - x$avgSeason)^2)

@
The MSE between the estimated and season ending averages for the James-Stein estimator and y was \Sexpr{mse.x}.

\item The James-Stein estimator is $z_{i}$ = $\bar{y} + c(y_{i} - \bar{y}$), where c was estimated to be \Sexpr{c.x} in this case. The average squared distance between the James-Stein estimator and the observed data is very small. Using the average of the 45 hits, c, and the overall average, the James-Stein estimator estimated the true season averages very accurately on average.
\end{enumerate}

\item%2
\begin{enumerate}
\item%2a
To generate my 18 baseball players, given that the typical batting averages are between 0.15 and 0.35, I will generate 18 random numbers from the UNIF(0.15,0.35) distribution.

<<prob2a>>=
set.seed(53228)
n <- 18
rand_avgs <- runif(n,0.15,0.35)
@

The realizations are \Sexpr{rand_avgs}.

\item%2b
To generate 45 at bats for each player, I will draw 45 times randomly from a uniform distribution that is the same width as above (0.2), but that has been shifted to be centered at the realized ``true average" for that player.

<<prob2b>>=
m <- 45

rand_45 <- sapply(rand_avgs, function(t){rbinom(45,1,t)})

rand_mean_45 <- apply(rand_45, 2, mean)

rand_mean_45
@

\item%2c

<<prob2c>>=

mse_rand_mean_45 <- sum((rand_45 - x$avgSeason)^2)
mse_rand_ <- sum((phat.x - x$avgSeason)^2)

  
phat_rand <- mean(rand45)
sigma2_rand45 <- phat_rand*(1-phat_rand)/m
c_rand <- 1-((dim(rand45)[1] - 3)*sigma2_rand/sum((rand45 - phat_rand)^2))

z_rand <- phat_rand + c_rand*(rand45 - phat_rand)


mse_rand <- sum((z_rand - rand_avgs)^2)
@

The MSE between the estimated and season ending averages for the James-Stein estimator and the observed average was \Sexpr{mse_rand}.

\item%2d
Repeat this entire procedure 1000 times and record the proportion of simulations where the James-Stein estimator is better.

{\bf How do we know if the James-Stein estimator is better based on the MSE?}
<<prob2d>>=

rand_avgs_new <- data.frame(replicate(1000, runif(n,0.15,0.35)))

rand45_new <- NULL
phat_rand_new <- NULL
sigma2_rand_new <- NULL
c_rand_new <- NULL
z_rand_new <- NULL
mse_rand_new <- NULL
#ask andy how to work with arrays here
for(i in 1:18){
  for(j in 1:1000){
rand45_new[i,j] <- sapply(rand_avgs_new[i,j],fnt_unif)
}}

phat_rand_new[i] <- mean(rand45_new[i])
sigma2_rand_new[i] <- phat_rand_new[i]*(1-phat_rand_new[i])/m

#something in c_rand_new is wrong
c_rand_new <- 1-((18 - 3)*sigma2_rand_new/sum((rand45_new[,1] - phat_rand_new)^2))

z_rand_new[i] <- phat_rand_new[i] + c_rand_new[i]*(rand45_new[i] - phat_rand_new[i])


mse_rand_new[i] <- sum((z_rand_new[i] - rand_avgs_new[i])^2)
}



@


<<prob1>>=
set.seed(10222016)
library(mnormt) # rmnorm, dmnorm
library(MCMCpack) #riwish
################################
###simulate multivariate normal data
################################
n=100
p=3
true.mu <- rep(0,p)

# create correlation matrix
rho.12 <- .7
rho.23 <- .3
rho.13 <- 0
R <- matrix(c(1,rho.12,rho.13,rho.12,1,
                  rho.23,rho.13,rho.23,1),p,p)

y.vec <- rmnorm(n,true.mu,R)
colMeans(y.vec)
cor(y.vec)
@


<<gibbs>>=

################################
# Run Gibbs Sampler
################################
###initialize 
num.sims <- 10000
mu <- matrix(0,nrow=num.sims,ncol=p)
Sigma <- array(0,dim=c(p,p,num.sims)) 
Sigma[,,1] <- diag(p)
# note when p is large it makes sense to only save unique
# elements in Sigma

###priors
# theta
mu.0 <- rep(0,p)
Lambda.0 <- matrix(.3,p,p)
diag(Lambda.0) <- 1
Lambda.0 <- Lambda.0 * 10000
Lambda.0.inv <- solve(Lambda.0)
# visualize a draws from prior
rmnorm(5,mu.0,Lambda.0)

# Sigma
nu.0 <- p 
nu.n <- nu.0 + n
S.0 <- matrix(.2,p,p)
diag(S.0) <- 1
# visualize a few draws from prior
riwish(p-1,solve(S.0)) # note nu.0 must be atleast p
riwish(p,solve(S.0)) 
riwish(nu.0,solve(S.0))

for (i in 2:num.sims){
  #sample mu
  Sig.inv <- solve(Sigma[,,(i-1)])
  Lambda.n <- solve(n* Sig.inv + Lambda.0.inv )
  mu.n <- Lambda.n %*% (Sig.inv %*% colSums(y.vec) + Lambda.0.inv %*% mu.0)
  mu[i,] <- rmnorm(1,mu.n,Lambda.n)
  
  # Sample Sigma
  mu.matrix <- matrix(mu[i,],n,p,byrow=T)
  S.theta <- t(y.vec - mu.matrix) %*% (y.vec - mu.matrix)
  Sigma[,,i] <- riwish(nu.n,(S.theta + S.0)) # note the parameterization
}

# look at theta
colMeans(mu)
colMeans(y.vec)
par(mfcol=c(1,3))
for (i in 1:p){
  plot(mu[,i],type='l',ylab=expression(mu[i]))
  abline(h=true.mu[i],col='red',lwd=2)
}

# look at Sigma
var(y.vec)
matrix(c(mean(Sigma[1,1,]),mean(Sigma[1,2,]),mean(Sigma[1,3,]),
            mean(Sigma[2,1,]),mean(Sigma[2,2,]),mean(Sigma[2,3,]),
            mean(Sigma[3,1,]),mean(Sigma[3,2,]),mean(Sigma[3,3,])),p,p)
par(mfcol=c(3,3))
for(i in 1:p){
  for  (j in 1:p){
    plot(Sigma[i,j,],type='l',ylab=expression(sigma[i,j]))
    abline(h=R[i,j],col='firebrick4',lwd=3)
  }
}
@


<<prob1>>=
require(LearnBayes)
require(truncnorm)
@

\end{enumerate}

\end{document}
